@article{isenseovereating,
title = "I sense overeating: Motif-based machine learning framework to detect overeating using wrist-worn sensing",
journal = "Information Fusion",
volume = "41",
pages = "37 - 47",
year = "2018",
issn = "1566-2535",
doi = "https://doi.org/10.1016/j.inffus.2017.08.003",
url = "http://www.sciencedirect.com/science/article/pii/S1566253517304785",
author = "Shibo Zhang and William Stogin and Nabil Alshurafa",
keywords = "Wrist-worn sensors, Wearables, Hand-to-mouth gestures, Overeating, Inertial sensors, Motif-based segmentation, K-Spectral Centroid Clustering, Fusion, Classification, Feeding gesture",
abstract = "Obesity, caused primarily by overeating, is a preventable chronic disease yielding staggering healthcare costs. To detect overeating passively, a machine learning framework was designed to detect and accurately count the number of feeding gestures during an eating episode to characterize each eating episode with a feeding gesture count using a 6-axis inertial wrist-worn sensor. Moreover, detecting feeding gestures is useful to aid in end-of-day dietary recalls. It has been shown that feeding gesture count correlates with caloric intake; the more one eats, the more calories one is likely consuming. Recent research has shown promise in passively detecting feeding gestures, but this effort focuses on bridging detection of feeding gesture count and identifying overeating episodes. This paper presents results on three experiments: highly structured (participants pretending to eat), in-lab structured with confounding activities (participants eating while performing other scripted activities), and unstructured overeating (participants induced to overeat while watching television and eating their favorite foods). Our experiment successfully induced overeating in 50% of the participants, showing a correlation between feeding gesture count and caloric intake in unstructured eating (r=.79, p-value=.007). Results provide an approximate upper bound on feeding gesture classification using exact segmentation techniques, and show improvement when compared to prior sliding window techniques. Results also suggest the importance of stressing the challenge of accurate segmentation over identifying the accurate classification technique in detection of feeding gestures. Since participant-dependent models provide optimal results, a motif-based time-point fusion classification (MTFC) framework is proposed using spectral energy density, K-Spectral Centroid Clustering, symbolic aggregate approximation (SAX), a Random Forest classifier (trained on segmented motifs) and a time-point classifier fusion technique to show reliable classification of feeding gestures (75% F-measure), and a 94% accuracy of feeding gesture count in the unstructured eating experiment, resulting in a root mean square error of 2.9 feeding gestures. Mapping feeding gesture count to caloric intake, we obtain a rough estimate of whether participants overate while watching television."
}